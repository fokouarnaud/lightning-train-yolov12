#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script d'export des modÃ¨les YOLOv12-Face
Export vers ONNX, TensorRT, CoreML, etc. pour production et mobile
"""

import os
import sys
import argparse
import logging
import time
from pathlib import Path
from typing import List, Dict, Any

# Ajouter le rÃ©pertoire parent au path
sys.path.append(str(Path(__file__).parent.parent))

from ultralytics import YOLO
import torch

def setup_logging(verbose: bool = False):
    """Configure le systÃ¨me de logging"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('export_models.log')
        ]
    )

def parse_args():
    """Parse les arguments de ligne de commande"""
    parser = argparse.ArgumentParser(
        description='Export des modÃ¨les YOLOv12-Face vers diffÃ©rents formats',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        '--model-path',
        type=str,
        required=True,
        help='Chemin vers le modÃ¨le PyTorch (.pt)'
    )
    
    parser.add_argument(
        '--formats',
        type=str,
        nargs='+',
        default=['onnx'],
        choices=['onnx', 'torchscript', 'coreml', 'tflite', 'engine', 'pb', 'saved_model'],
        help='Formats d\'export Ã  gÃ©nÃ©rer'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default='./outputs/exports',
        help='RÃ©pertoire de sortie pour les modÃ¨les exportÃ©s'
    )
    
    parser.add_argument(
        '--img-size',
        type=int,
        default=640,
        help='Taille des images d\'entrÃ©e'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=1,
        help='Taille de batch pour l\'export'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        default='cpu',
        choices=['cpu', 'cuda', 'mps'],
        help='Device pour l\'export'
    )
    
    parser.add_argument(
        '--half',
        action='store_true',
        help='Utiliser la prÃ©cision FP16'
    )
    
    parser.add_argument(
        '--dynamic',
        action='store_true',
        help='Export avec tailles dynamiques (ONNX)'
    )
    
    parser.add_argument(
        '--simplify',
        action='store_true',
        default=True,
        help='Simplifier le modÃ¨le ONNX'
    )
    
    parser.add_argument(
        '--optimize-for-mobile',
        action='store_true',
        help='Optimisations spÃ©cifiques mobile'
    )
    
    parser.add_argument(
        '--benchmark',
        action='store_true',
        help='Benchmarker les modÃ¨les exportÃ©s'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Mode verbose avec plus de logs'
    )
    
    return parser.parse_args()

def validate_model_path(model_path: str) -> bool:
    """Valide le chemin du modÃ¨le"""
    logger = logging.getLogger(__name__)
    
    path = Path(model_path)
    
    if not path.exists():
        logger.error(f"âŒ ModÃ¨le non trouvÃ©: {model_path}")
        return False
    
    if not path.suffix == '.pt':
        logger.error(f"âŒ Format de modÃ¨le non supportÃ©: {path.suffix}")
        return False
    
    # VÃ©rifier que le fichier n'est pas vide
    if path.stat().st_size == 0:
        logger.error(f"âŒ Fichier modÃ¨le vide: {model_path}")
        return False
    
    logger.info(f"âœ… ModÃ¨le valide: {model_path}")
    return True

def export_onnx(model: YOLO, output_dir: Path, args: argparse.Namespace) -> Dict[str, Any]:
    """
    Export vers ONNX
    
    Args:
        model: ModÃ¨le YOLO
        output_dir: RÃ©pertoire de sortie
        args: Arguments de ligne de commande
        
    Returns:
        Informations sur l'export
    """
    logger = logging.getLogger(__name__)
    logger.info("ðŸ“¦ Export ONNX...")
    
    try:
        start_time = time.time()
        
        # ParamÃ¨tres d'export ONNX
        export_args = {
            'format': 'onnx',
            'imgsz': args.img_size,
            'device': args.device,
            'half': args.half,
            'dynamic': args.dynamic,
            'simplify': args.simplify,
            'opset': 12,  # Version ONNX
        }
        
        # Optimisations pour mobile
        if args.optimize_for_mobile:
            export_args.update({
                'opset': 11,  # Meilleure compatibilitÃ© mobile
                'simplify': True,
                'dynamic': False,  # Tailles fixes pour mobile
            })
        
        # Export
        exported_model = model.export(**export_args)
        export_time = time.time() - start_time
        
        # Informations sur le fichier exportÃ©
        model_path = Path(exported_model)
        model_size = model_path.stat().st_size / (1024 * 1024)  # MB
        
        logger.info(f"âœ… Export ONNX terminÃ©: {model_path.name}")
        logger.info(f"   Taille: {model_size:.2f} MB")
        logger.info(f"   Temps: {export_time:.2f}s")
        
        return {
            'format': 'onnx',
            'path': str(model_path),
            'size_mb': model_size,
            'export_time': export_time,
            'success': True
        }
        
    except Exception as e:
        logger.error(f"âŒ Erreur export ONNX: {e}")
        return {
            'format': 'onnx',
            'success': False,
            'error': str(e)
        }

def export_torchscript(model: YOLO, output_dir: Path, args: argparse.Namespace) -> Dict[str, Any]:
    """Export vers TorchScript"""
    logger = logging.getLogger(__name__)
    logger.info("ðŸ“¦ Export TorchScript...")
    
    try:
        start_time = time.time()
        
        exported_model = model.export(
            format='torchscript',
            imgsz=args.img_size,
            device=args.device,
            half=args.half
        )
        
        export_time = time.time() - start_time
        model_path = Path(exported_model)
        model_size = model_path.stat().st_size / (1024 * 1024)
        
        logger.info(f"âœ… Export TorchScript terminÃ©: {model_path.name}")
        logger.info(f"   Taille: {model_size:.2f} MB")
        logger.info(f"   Temps: {export_time:.2f}s")
        
        return {
            'format': 'torchscript',
            'path': str(model_path),
            'size_mb': model_size,
            'export_time': export_time,
            'success': True
        }
        
    except Exception as e:
        logger.error(f"âŒ Erreur export TorchScript: {e}")
        return {
            'format': 'torchscript',
            'success': False,
            'error': str(e)
        }

def export_coreml(model: YOLO, output_dir: Path, args: argparse.Namespace) -> Dict[str, Any]:
    """Export vers CoreML (iOS)"""
    logger = logging.getLogger(__name__)
    logger.info("ðŸ“¦ Export CoreML...")
    
    try:
        start_time = time.time()
        
        # CoreML est optimisÃ© pour iOS
        exported_model = model.export(
            format='coreml',
            imgsz=args.img_size,
            device='cpu',  # CoreML nÃ©cessite CPU
            half=False,    # CoreML gÃ¨re automatiquement la prÃ©cision
            nms=True       # Inclure NMS dans le modÃ¨le
        )
        
        export_time = time.time() - start_time
        model_path = Path(exported_model)
        
        # CoreML produit un dossier .mlmodel
        if model_path.is_dir():
            model_size = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file()) / (1024 * 1024)
        else:
            model_size = model_path.stat().st_size / (1024 * 1024)
        
        logger.info(f"âœ… Export CoreML terminÃ©: {model_path.name}")
        logger.info(f"   Taille: {model_size:.2f} MB")
        logger.info(f"   Temps: {export_time:.2f}s")
        
        return {
            'format': 'coreml',
            'path': str(model_path),
            'size_mb': model_size,
            'export_time': export_time,
            'success': True
        }
        
    except Exception as e:
        logger.error(f"âŒ Erreur export CoreML: {e}")
        return {
            'format': 'coreml',
            'success': False,
            'error': str(e)
        }

def export_tflite(model: YOLO, output_dir: Path, args: argparse.Namespace) -> Dict[str, Any]:
    """Export vers TensorFlow Lite (Android)"""
    logger = logging.getLogger(__name__)
    logger.info("ðŸ“¦ Export TensorFlow Lite...")
    
    try:
        start_time = time.time()
        
        # TFLite pour Android
        export_args = {
            'format': 'tflite',
            'imgsz': args.img_size,
            'device': 'cpu',
            'int8': args.optimize_for_mobile,  # Quantification INT8 pour mobile
        }
        
        exported_model = model.export(**export_args)
        export_time = time.time() - start_time
        model_path = Path(exported_model)
        model_size = model_path.stat().st_size / (1024 * 1024)
        
        logger.info(f"âœ… Export TFLite terminÃ©: {model_path.name}")
        logger.info(f"   Taille: {model_size:.2f} MB")
        logger.info(f"   Temps: {export_time:.2f}s")
        
        return {
            'format': 'tflite',
            'path': str(model_path),
            'size_mb': model_size,
            'export_time': export_time,
            'success': True
        }
        
    except Exception as e:
        logger.error(f"âŒ Erreur export TFLite: {e}")
        return {
            'format': 'tflite',
            'success': False,
            'error': str(e)
        }

def benchmark_model(model_path: str, img_size: int, device: str) -> Dict[str, float]:
    """
    Benchmark d'un modÃ¨le exportÃ©
    
    Args:
        model_path: Chemin vers le modÃ¨le
        img_size: Taille des images
        device: Device de test
        
    Returns:
        MÃ©triques de performance
    """
    logger = logging.getLogger(__name__)
    logger.info(f"â±ï¸ Benchmark: {Path(model_path).name}")
    
    try:
        import numpy as np
        
        # CrÃ©er des donnÃ©es de test
        dummy_input = np.random.rand(1, 3, img_size, img_size).astype(np.float32)
        
        # Benchmark selon le format
        if model_path.endswith('.onnx'):
            return benchmark_onnx(model_path, dummy_input)
        elif model_path.endswith('.pt'):
            return benchmark_torchscript(model_path, dummy_input, device)
        else:
            logger.warning(f"âš ï¸ Benchmark non supportÃ© pour: {model_path}")
            return {}
            
    except Exception as e:
        logger.error(f"âŒ Erreur benchmark: {e}")
        return {}

def benchmark_onnx(model_path: str, dummy_input: np.ndarray) -> Dict[str, float]:
    """Benchmark d'un modÃ¨le ONNX"""
    try:
        import onnxruntime as ort
        
        # CrÃ©er la session ONNX
        session = ort.InferenceSession(model_path)
        input_name = session.get_inputs()[0].name
        
        # Warmup
        for _ in range(5):
            session.run(None, {input_name: dummy_input})
        
        # Benchmark
        times = []
        for _ in range(20):
            start = time.time()
            session.run(None, {input_name: dummy_input})
            times.append(time.time() - start)
        
        avg_time = np.mean(times) * 1000  # ms
        fps = 1000 / avg_time
        
        return {
            'avg_inference_ms': avg_time,
            'fps': fps,
            'min_inference_ms': min(times) * 1000,
            'max_inference_ms': max(times) * 1000
        }
        
    except ImportError:
        logging.getLogger(__name__).warning("âš ï¸ onnxruntime non installÃ©")
        return {}
    except Exception as e:
        logging.getLogger(__name__).error(f"âŒ Erreur benchmark ONNX: {e}")
        return {}

def benchmark_torchscript(model_path: str, dummy_input: np.ndarray, device: str) -> Dict[str, float]:
    """Benchmark d'un modÃ¨le TorchScript"""
    try:
        # Charger le modÃ¨le TorchScript
        model = torch.jit.load(model_path, map_location=device)
        model.eval()
        
        # Convertir l'input
        tensor_input = torch.from_numpy(dummy_input).to(device)
        
        # Warmup
        with torch.no_grad():
            for _ in range(5):
                model(tensor_input)
        
        # Benchmark
        times = []
        with torch.no_grad():
            for _ in range(20):
                start = time.time()
                model(tensor_input)
                if device == 'cuda':
                    torch.cuda.synchronize()
                times.append(time.time() - start)
        
        avg_time = np.mean(times) * 1000  # ms
        fps = 1000 / avg_time
        
        return {
            'avg_inference_ms': avg_time,
            'fps': fps,
            'min_inference_ms': min(times) * 1000,
            'max_inference_ms': max(times) * 1000
        }
        
    except Exception as e:
        logging.getLogger(__name__).error(f"âŒ Erreur benchmark TorchScript: {e}")
        return {}

def main():
    """Fonction principale"""
    args = parse_args()
    
    print("ðŸ“¦ YOLOv12-Face Model Exporter")
    print("=" * 50)
    
    setup_logging(args.verbose)
    logger = logging.getLogger(__name__)
    
    # Valider le modÃ¨le d'entrÃ©e
    if not validate_model_path(args.model_path):
        return 1
    
    # CrÃ©er le rÃ©pertoire de sortie
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Charger le modÃ¨le
    try:
        logger.info(f"ðŸ”„ Chargement du modÃ¨le: {args.model_path}")
        model = YOLO(args.model_path)
        logger.info("âœ… ModÃ¨le chargÃ©")
    except Exception as e:
        logger.error(f"âŒ Erreur chargement modÃ¨le: {e}")
        return 1
    
    # Mappage des fonctions d'export
    export_functions = {
        'onnx': export_onnx,
        'torchscript': export_torchscript,
        'coreml': export_coreml,
        'tflite': export_tflite,
    }
    
    # Exporter vers les formats demandÃ©s
    export_results = []
    
    for format_name in args.formats:
        logger.info(f"\nðŸ“¦ Export {format_name.upper()}...")
        
        if format_name in export_functions:
            result = export_functions[format_name](model, output_dir, args)
            export_results.append(result)
        else:
            logger.warning(f"âš ï¸ Format non supportÃ©: {format_name}")
            try:
                # Essayer l'export gÃ©nÃ©rique ultralytics
                exported_model = model.export(
                    format=format_name,
                    imgsz=args.img_size,
                    device=args.device,
                    half=args.half
                )
                
                model_path = Path(exported_model)
                model_size = model_path.stat().st_size / (1024 * 1024)
                
                result = {
                    'format': format_name,
                    'path': str(model_path),
                    'size_mb': model_size,
                    'success': True
                }
                export_results.append(result)
                logger.info(f"âœ… Export {format_name} terminÃ©")
                
            except Exception as e:
                logger.error(f"âŒ Erreur export {format_name}: {e}")
                export_results.append({
                    'format': format_name,
                    'success': False,
                    'error': str(e)
                })
    
    # Benchmark des modÃ¨les exportÃ©s
    if args.benchmark:
        logger.info("\nâ±ï¸ Benchmark des modÃ¨les exportÃ©s...")
        for result in export_results:
            if result.get('success') and 'path' in result:
                bench_result = benchmark_model(
                    result['path'], 
                    args.img_size, 
                    args.device
                )
                result.update(bench_result)
    
    # RÃ©sumÃ© final
    print("\n" + "=" * 50)
    print("ðŸ“‹ RÃ‰SUMÃ‰ DES EXPORTS")
    print("=" * 50)
    
    successful_exports = 0
    total_exports = len(export_results)
    
    for result in export_results:
        format_name = result['format'].upper()
        
        if result.get('success'):
            successful_exports += 1
            print(f"âœ… {format_name}")
            print(f"   ðŸ“ {result.get('path', 'N/A')}")
            print(f"   ðŸ“Š {result.get('size_mb', 0):.2f} MB")
            
            if 'avg_inference_ms' in result:
                print(f"   â±ï¸  {result['avg_inference_ms']:.2f}ms ({result['fps']:.1f} FPS)")
        else:
            print(f"âŒ {format_name}")
            print(f"   Erreur: {result.get('error', 'Inconnue')}")
        print()
    
    print(f"ðŸ“Š SuccÃ¨s: {successful_exports}/{total_exports}")
    
    if successful_exports > 0:
        print("\nðŸ“ Prochaines Ã©tapes:")
        print("1. Tester les modÃ¨les exportÃ©s")
        print("2. IntÃ©grer dans votre application")
        print("3. VÃ©rifier les performances en production")
    
    print("=" * 50)
    return 0 if successful_exports > 0 else 1

if __name__ == "__main__":
    sys.exit(main())

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de test rapide pour YOLOv12-Face Lightning.ai
VÃ©rifie que l'installation et la configuration sont correctes
"""

import sys
import os
import time
import traceback
from pathlib import Path

def test_imports():
    """Teste tous les imports nÃ©cessaires"""
    print("ğŸ” Test des imports...")
    
    tests = [
        ("Python standard", ["os", "sys", "pathlib", "time", "json", "yaml"]),
        ("Scientifique", ["numpy", "pandas", "matplotlib", "seaborn"]),
        ("Vision", ["cv2", "PIL"]),
        ("Deep Learning", ["torch", "torchvision"]),
        ("YOLO", ["ultralytics"]),
        ("Projet", ["src"])
    ]
    
    results = {}
    
    for category, modules in tests:
        print(f"  ğŸ“¦ {category}:")
        category_results = []
        
        for module in modules:
            try:
                if module == "src":
                    # Test import du projet
                    sys.path.append(str(Path(__file__).parent))
                    import src
                    from src import YOLOv12FaceTrainer, DataManager, ModelManager
                else:
                    __import__(module)
                
                print(f"    âœ… {module}")
                category_results.append(True)
                
            except ImportError as e:
                print(f"    âŒ {module}: {e}")
                category_results.append(False)
            except Exception as e:
                print(f"    âš ï¸ {module}: {e}")
                category_results.append(False)
        
        results[category] = category_results
    
    # RÃ©sumÃ©
    total_tests = sum(len(tests) for _, tests in tests)
    passed_tests = sum(sum(results.values(), []))
    
    print(f"\nğŸ“Š RÃ©sumÃ© imports: {passed_tests}/{total_tests} rÃ©ussis")
    return passed_tests == total_tests

def test_gpu():
    """Teste la disponibilitÃ© du GPU"""
    print("\nğŸš€ Test GPU...")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0)
            memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            print(f"  âœ… GPU disponible: {device_name}")
            print(f"  ğŸ“Š Nombre de GPUs: {device_count}")
            print(f"  ğŸ’¾ MÃ©moire GPU: {memory:.1f}GB")
            
            # Test simple
            x = torch.randn(100, 100).cuda()
            y = torch.mm(x, x)
            print(f"  âœ… Test calcul GPU rÃ©ussi")
            
            return True
            
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print(f"  âœ… Apple Silicon GPU (MPS) disponible")
            return True
            
        else:
            print(f"  âš ï¸ Aucun GPU dÃ©tectÃ©, utilisation CPU")
            return False
            
    except Exception as e:
        print(f"  âŒ Erreur test GPU: {e}")
        return False

def test_directories():
    """Teste la structure des dossiers"""
    print("\nğŸ“ Test structure des dossiers...")
    
    required_dirs = [
        "src",
        "configs", 
        "scripts",
        "notebooks",
        "datasets",
        "outputs"
    ]
    
    base_dir = Path(__file__).parent
    results = []
    
    for dir_name in required_dirs:
        dir_path = base_dir / dir_name
        if dir_path.exists():
            print(f"  âœ… {dir_name}/")
            results.append(True)
        else:
            print(f"  âŒ {dir_name}/ manquant")
            results.append(False)
    
    # CrÃ©er les dossiers manquants
    missing_dirs = [required_dirs[i] for i, exists in enumerate(results) if not exists]
    if missing_dirs:
        print(f"  ğŸ”§ CrÃ©ation des dossiers manquants...")
        for dir_name in missing_dirs:
            (base_dir / dir_name).mkdir(parents=True, exist_ok=True)
            print(f"    âœ… {dir_name}/ crÃ©Ã©")
    
    return True

def test_config():
    """Teste le chargement des configurations"""
    print("\nâš™ï¸ Test configurations...")
    
    config_files = ["config.yaml", "configs/quick_test.yaml", "configs/production.yaml"]
    base_dir = Path(__file__).parent
    
    try:
        import yaml
        
        for config_file in config_files:
            config_path = base_dir / config_file
            
            if config_path.exists():
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = yaml.safe_load(f)
                    print(f"  âœ… {config_file}")
                except Exception as e:
                    print(f"  âŒ {config_file}: {e}")
                    return False
            else:
                print(f"  âš ï¸ {config_file}: non trouvÃ©")
        
        return True
        
    except ImportError:
        print(f"  âŒ PyYAML non disponible")
        return False

def test_ultralytics():
    """Teste l'installation d'Ultralytics YOLO"""
    print("\nğŸ¯ Test Ultralytics YOLO...")
    
    try:
        from ultralytics import YOLO
        
        # Test de crÃ©ation d'un modÃ¨le simple
        print("  ğŸ”„ Test crÃ©ation modÃ¨le...")
        model = YOLO('yolov8n.pt')  # ModÃ¨le lÃ©ger pour test
        print("  âœ… ModÃ¨le YOLO crÃ©Ã©")
        
        # Test d'information du modÃ¨le
        try:
            info = model.info()
            print("  âœ… Informations modÃ¨le rÃ©cupÃ©rÃ©es")
        except:
            print("  âš ï¸ Info modÃ¨le non disponible (normal)")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Erreur Ultralytics: {e}")
        return False

def test_project_modules():
    """Teste les modules du projet"""
    print("\nğŸ—ï¸ Test modules du projet...")
    
    try:
        sys.path.append(str(Path(__file__).parent))
        
        # Test DataManager
        print("  ğŸ”„ Test DataManager...")
        from src.data_manager import DataManager
        
        data_config = {
            'dataset': 'widerface',
            'path': './datasets'
        }
        data_manager = DataManager(data_config)
        print("  âœ… DataManager initialisÃ©")
        
        # Test ModelManager
        print("  ğŸ”„ Test ModelManager...")
        from src.model_manager import ModelManager
        
        model_config = {
            'size': 's',
            'num_classes': 1,
            'class_names': ['face']
        }
        model_manager = ModelManager(model_config)
        print("  âœ… ModelManager initialisÃ©")
        
        # Test LightningLogger
        print("  ğŸ”„ Test LightningLogger...")
        from src.lightning_utils import LightningLogger
        
        logger = LightningLogger("test", 10)
        print("  âœ… LightningLogger initialisÃ©")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Erreur modules projet: {e}")
        traceback.print_exc()
        return False

def test_quick_run():
    """Teste un run rapide du pipeline"""
    print("\nâš¡ Test run rapide...")
    
    try:
        # Test du script principal
        main_script = Path(__file__).parent / "lightning_main.py"
        
        if main_script.exists():
            print("  âœ… Script principal trouvÃ©")
            
            # Test import du script principal
            sys.path.append(str(Path(__file__).parent))
            import lightning_main
            print("  âœ… Script principal importÃ©")
            
            return True
        else:
            print("  âŒ Script principal non trouvÃ©")
            return False
            
    except Exception as e:
        print(f"  âŒ Erreur test run: {e}")
        return False

def create_test_report(results):
    """CrÃ©e un rapport de test"""
    print("\nğŸ“‹ RAPPORT DE TEST")
    print("=" * 50)
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    
    print(f"Tests rÃ©ussis: {passed_tests}/{total_tests}")
    print(f"Taux de rÃ©ussite: {passed_tests/total_tests*100:.1f}%")
    
    print("\nDÃ©tail:")
    for test_name, result in results.items():
        status = "âœ…" if result else "âŒ"
        print(f"  {status} {test_name}")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ TOUS LES TESTS RÃ‰USSIS!")
        print("âœ… Le projet est prÃªt Ã  Ãªtre utilisÃ©")
        return True
    else:
        print(f"\nâš ï¸ {total_tests - passed_tests} tests Ã©chouÃ©s")
        print("ğŸ”§ VÃ©rifiez les erreurs ci-dessus")
        return False

def main():
    """Fonction principale de test"""
    print("ğŸ§ª YOLOv12-Face Lightning.ai - Tests SystÃ¨me")
    print("=" * 60)
    
    start_time = time.time()
    
    # Liste des tests Ã  exÃ©cuter
    tests = [
        ("Imports", test_imports),
        ("GPU", test_gpu), 
        ("Dossiers", test_directories),
        ("Configurations", test_config),
        ("Ultralytics", test_ultralytics),
        ("Modules Projet", test_project_modules),
        ("Run Rapide", test_quick_run)
    ]
    
    results = {}
    
    # ExÃ©cuter tous les tests
    for test_name, test_func in tests:
        try:
            result = test_func()
            results[test_name] = result
        except Exception as e:
            print(f"âŒ Erreur inattendue dans {test_name}: {e}")
            results[test_name] = False
    
    # CrÃ©er le rapport
    success = create_test_report(results)
    
    # Temps d'exÃ©cution
    duration = time.time() - start_time
    print(f"\nâ±ï¸ Tests terminÃ©s en {duration:.2f}s")
    
    if success:
        print("\nğŸš€ Prochaines Ã©tapes:")
        print("1. python scripts/setup_environment.py")
        print("2. python scripts/download_datasets.py --dataset widerface")
        print("3. python lightning_main.py --config configs/quick_test.yaml")
    
    return 0 if success else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
